{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394fc3ab-ba00-43ef-a4cb-324e026a2890",
   "metadata": {},
   "source": [
    "# Enhancing Academic Success: The Impact of Artificial Intelligence on College Student Performance \n",
    "## (FEPS - Cairo University - Statistics Department - Arabic Section)\n",
    "\n",
    "#### Prepared by:\n",
    "#### Mohamed Gamal Sewid\n",
    "#### Mahmoud Ali Abdulkadir\n",
    "\n",
    "#### Under the supervision of:\n",
    "#### Dr. Amani Abu Bakr\n",
    "---\n",
    "\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook documents the data analysis steps for our project **\"Enhancing Academic Success: The Impact of Artificial Intelligence on College Student Performance.\"**\n",
    "\n",
    "The study aims to examine how AI usage affects student performance among different groups at the Faculty of Economics and Political Science (FEPS), Cairo University.\n",
    "\n",
    "The workflow consists of three major phases:\n",
    "- **Phase 1**: Data Cleaning and Oversampling\n",
    "- **Phase 2**: Descriptive Analysis\n",
    "- **Phase 3**: Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "# PHASE 1: Data Cleaning\n",
    "\n",
    "## 1. Columns Titles Coding\n",
    "\n",
    "The columns titles were changed according to a codes sheet agreed upon between the team members to ease the future coding of this dataset.\n",
    "\n",
    "| Code   | Question                                                                                  |\n",
    "|--------|-------------------------------------------------------------------------------------------|\n",
    "| Q1     | النوع:                                                                                   |\n",
    "| Q2     | السنة الدراسية:                                                                           |\n",
    "| Q3     | الشعبة الدراسية:                                                                          |\n",
    "| Q4     | التخصص الأكاديمي:                                                                         |\n",
    "| Q5     | ما نطاق المعدل التراكمي (GPA) الخاص بك؟                                                   |\n",
    "| Q6     | في المتوسط، كم ساعة يوميًا تستخدم الأجهزة التكنولوجية مثل الهواتف المحمولة أو أجهزة اللابتوب؟ |\n",
    "| Q7     | هل استخدمت أدوات الذكاء الاصطناعي مثل ChatGPT في دراستك الجامعية؟                          |\n",
    "| Q8     | ما أدوات الذكاء الاصطناعي التي تستخدمها بشكل متكرر؟                                       |\n",
    "| Q9     | ما مدى ثقتك في استخدام أدوات الذكاء الاصطناعي في دراستك الجامعية؟                           |\n",
    "| Q10    | ما مدى قدرتك على استخدام أدوات الذكاء الاصطناعي في أبحاثك ودراستك؟                         |\n",
    "| Q11    | منذ متى وأنت تستخدم أدوات الذكاء الاصطناعي في دراستك الجامعية؟                              |\n",
    "| Q12    | إلى أي مدى تستخدم أدوات الذكاء الاصطناعي في دراستك الجامعية؟                                |\n",
    "| Q13_1  | استخدام أدوات الذكاء الاصطناعي [حل المسائل الرياضية]                                        |\n",
    "| Q13_2  | استخدام أدوات الذكاء الاصطناعي [الترجمة]                                                    |\n",
    "| Q13_3  | استخدام أدوات الذكاء الاصطناعي [الكتابة الأكاديمية]                                         |\n",
    "| Q13_4  | استخدام أدوات الذكاء الاصطناعي [تلخيص النصوص]                                               |\n",
    "| Q13_5  | استخدام أدوات الذكاء الاصطناعي [البحث عن المعلومات]                                          |\n",
    "| Q13_6  | استخدام أدوات الذكاء الاصطناعي [البرمجة]                                                     |\n",
    "| Q13_7  | استخدام أدوات الذكاء الاصطناعي [إنشاء العروض التقديمية]                                      |\n",
    "| Q14    | إلى أي مدى أثر استخدامك لأدوات الذكاء الاصطناعي على معدلك التراكمي؟                         |\n",
    "| Q15    | كيف كان تأثير الذكاء الاصطناعي على جودة واجباتك الدراسية؟                                   |\n",
    "| Q16    | كيف أثر استخدامك للذكاء الاصطناعي على تطوير مهاراتك البحثية؟                                |\n",
    "| Q17    | ما مدى اعتمادك على الذكاء الاصطناعي في الدراسة والاستعداد للامتحانات؟                        |\n",
    "| Q18    | ما مدى دعم أساتذتك لاستخدام الذكاء الاصطناعي في التعليم؟                                    |\n",
    "| Q19    | إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في توفير الوقت أثناء دراستك؟                       |\n",
    "| Q20    | إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهارات التفكير النقدي لديك؟               |\n",
    "| Q21    | إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهارات الكتابة الأكاديمية لديك؟           |\n",
    "| Q22    | إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهارات التعلم الذاتي؟                     |\n",
    "| Q23    | إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهاراتك في التعاون والعمل الجماعي؟        |\n",
    "| Q24    | إلى أي مدى ساهمت أدوات الذكاء الاصطناعي في زيادة رضاك عن أدائك الأكاديمي؟                    |\n",
    "| Q25    | إلى أي مدى تواجه مشكلة بطء الحصول على النتائج عند استخدام أدوات الذكاء الاصطناعي؟           |\n",
    "| Q26    | ما مدى قلقك بشأن المعلومات المضللة أو غير الدقيقة الناتجة عن الذكاء الاصطناعي؟              |\n",
    "| Q27    | إلى أي مدى تواجه صعوبة في تشغيل أدوات الذكاء الاصطناعي على الأنظمة أو المنصات؟               |\n",
    "| Q28    | إلى أي مدى لديك مخاوف أخلاقية بشأن استخدام الذكاء الاصطناعي في العملية التعليمية؟           |\n",
    "| Q29    | هل تستخدم أدوات الذكاء الاصطناعي المدفوعة؟                                                 |\n",
    "| Q30    | كيف تُقيّم تكلفة أدوات الذكاء الاصطناعي المدفوعة مقارنة بالفوائد؟                           |\n",
    "| Q31    | إن كنت بحاجة إلى تدريب حول كيفية استخدام الذكاء الاصطناعي، فما الأسلوب الذي تفضله؟           |\n",
    "| Q32    | ما السياسات التي ينبغي لكلية الاقتصاد والعلوم السياسية تبنيها بشأن استخدام الذكاء الاصطناعي؟ |\n",
    "| Q33    | ما المستوى الذي تفضله لدمج أدوات الذكاء الاصطناعي في التعليم؟                              |\n",
    "| Q34    | هل لديك أي اقتراحات أو ملاحظات حول استخدام الذكاء الاصطناعي في العملية التعليمية؟           |\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Cleaning\n",
    "Below are the steps that were used to do the data cleaning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262d5cb-bfec-49ab-b7cc-36b8a34972f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import chi2_contingency\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feac4be-c46f-4752-8a2e-8938bfaf50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_excel('FINAL2025.xlsx')\n",
    "\n",
    "# Check\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a0193-d5c3-4c6d-bb2f-9130416c6d37",
   "metadata": {},
   "source": [
    "#### Coding the headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f03d3-dddc-425d-8b5c-8d34f91a4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the timestamp column\n",
    "df.drop(columns=[\"طابع زمني\"], inplace=True)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df.columns = df.columns.str.replace('\\n', '', regex=False)\n",
    "df.columns = df.columns.str.replace('\\u200f', '', regex=False)\n",
    "df.columns = df.columns.str.replace('\"', '', regex=False)\n",
    "\n",
    "# Remove leading numbers and dashes (like \"1. \" or \"13-  \")\n",
    "df.columns = df.columns.str.replace(r'^\\d+\\s*[-.]\\s*', '', regex=True)\n",
    "\n",
    "# Corrected dictionary with numbers removed, matches cleaned headers\n",
    "rename_dict = {\n",
    "    \"النوع:\": \"Q1\",\n",
    "    \"السنة الدراسية:\": \"Q2\",\n",
    "    \"الشعبة الدراسية:\": \"Q3\",\n",
    "    \"التخصص الأكاديمي:\": \"Q4\",\n",
    "    \"ما نطاق المعدل التراكمي (GPA) الخاص بك؟\": \"Q5\",\n",
    "    \"في المتوسط، كم ساعة يوميًا تستخدم الأجهزة التكنولوجية مثل الهواتف المحمولة، أجهزة اللابتوب، أو الأجهزة اللوحية؟\": \"Q6\",\n",
    "    \"هل استخدمت أدوات الذكاء الاصطناعي مثل ChatGPT في دراستك الجامعية؟\": \"Q7\",\n",
    "    \"ما أدوات الذكاء الاصطناعي التي تستخدمها بشكل متكرر؟\": \"Q8\",\n",
    "    \"ما مدى ثقتك في استخدام أدوات الذكاء الاصطناعي في دراستك الجامعية؟\": \"Q9\",\n",
    "    \"ما مدى قدرتك على استخدام أدوات الذكاء الاصطناعي في أبحاثك ودراستك؟\": \"Q10\",\n",
    "    \"منذ متى وأنت تستخدم أدوات الذكاء الاصطناعي في دراستك الجامعية؟\": \"Q11\",\n",
    "    \"إلى أي مدى تستخدم أدوات الذكاء الاصطناعي في دراستك الجامعية؟\": \"Q12\",\n",
    "    \"ما مدى تكرارك لاستخدام أدوات الذكاء الاصطناعي في المهام أو الأنشطة التالية أثناء دراستك؟ [حل المسائل الرياضية]\": \"Q13_1\",\n",
    "    \"ما مدى تكرارك لاستخدام أدوات الذكاء الاصطناعي في المهام أو الأنشطة التالية أثناء دراستك؟ [الترجمة]\": \"Q13_2\",\n",
    "    \"ما مدى تكرارك لاستخدام أدوات الذكاء الاصطناعي في المهام أو الأنشطة التالية أثناء دراستك؟ [الكتابة الأكاديمية]\": \"Q13_3\",\n",
    "    \"ما مدى تكرارك لاستخدام أدوات الذكاء الاصطناعي في المهام أو الأنشطة التالية أثناء دراستك؟ [تلخيص النصوص]\": \"Q13_4\",\n",
    "    \"ما مدى تكرارك لاستخدام أدوات الذكاء الاصطناعي في المهام أو الأنشطة التالية أثناء دراستك؟ [البحث عن المعلومات]\": \"Q13_5\",\n",
    "    \"ما مدى تكرارك لاستخدام أدوات الذكاء الاصطناعي في المهام أو الأنشطة التالية أثناء دراستك؟ [البرمجة]\": \"Q13_6\",\n",
    "    \"ما مدى تكرارك لاستخدام أدوات الذكاء الاصطناعي في المهام أو الأنشطة التالية أثناء دراستك؟ [إنشاء العروض التقديمية]\": \"Q13_7\",\n",
    "    \"إلى أي مدى أثر استخدامك لأدوات الذكاء الاصطناعي على معدلك التراكمي؟ (1 تعني انخفض بشكل كبير، 3 تعني لم يتغير، و5 تعني ارتفع بشكل كبير)\": \"Q14\",\n",
    "    \"كيف كان تأثير الذكاء الاصطناعي على جودة واجباتك الدراسية؟\": \"Q15\",\n",
    "    \"كيف أثر استخدامك للذكاء الاصطناعي على تطوير مهاراتك البحثية؟\": \"Q16\",\n",
    "    \"ما مدى اعتمادك على الذكاء الاصطناعي في الدراسة والاستعداد للامتحانات؟\": \"Q17\",\n",
    "    \"ما مدى دعم أساتذتك لاستخدام الذكاء الاصطناعي في التعليم؟\": \"Q18\",\n",
    "    \"إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في توفير الوقت أثناء دراستك؟\": \"Q19\",\n",
    "    \"إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهارات التفكير النقدي لديك؟\": \"Q20\",\n",
    "    \"إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهارات الكتابة الأكاديمية لديك؟\": \"Q21\",\n",
    "    \"إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهارات التعلم الذاتي؟\": \"Q22\",\n",
    "    \"إلى أي مدى ساعدتك أدوات الذكاء الاصطناعي في تحسين مهاراتك في التعاون والعمل الجماعي أثناء دراستك؟\": \"Q23\",\n",
    "    \"إلى أي مدى ساهمت أدوات الذكاء الاصطناعي في زيادة رضاك عن أدائك الأكاديمي خلال دراستك؟\": \"Q24\",\n",
    "    \"إلى أي مدى تواجه مشكلة بطء الحصول على النتائج عند استخدام أدوات الذكاء الاصطناعي؟\": \"Q25\",\n",
    "    \"ما مدى قلقك بشأن المعلومات المضللة أو غير الدقيقة الناتجة عن الذكاء الاصطناعي؟\": \"Q26\",\n",
    "    \"إلى أي مدى تواجه صعوبة في تشغيل أدوات الذكاء الاصطناعي على الأنظمة أو المنصات التي تستخدمها؟\": \"Q27\",\n",
    "    \"إلى أي مدى لديك مخاوف أخلاقية (مثل الخصوصية وأمان البيانات) بشأن استخدام الذكاء الاصطناعي في العملية التعليمية؟\": \"Q28\",\n",
    "    \"هل تستخدم أدوات الذكاء الاصطناعي المدفوعة؟\": \"Q29\",\n",
    "    \"إذا كنت تستخدم أدوات الذكاء الاصطناعي المدفوعة، كيف تُقيّم تكلفتها (بمتوسط اشتراك شهري بقيمة 1000 جنيه) مقارنة بالفوائد التي تحصل عليها منها؟\": \"Q30\",\n",
    "    \"إن كنت بحاجة إلى تدريب حول كيفية استخدام الذكاء الاصطناعي بفعالية، فما الأسلوب الذي تفضله لتلقي التدريب؟\": \"Q31\",\n",
    "    \"ما السياسات التي ينبغي لكلية الاقتصاد والعلوم السياسية بجامعة القاهرة تبنيها بشأن استخدام الذكاء الاصطناعي؟\": \"Q32\",\n",
    "    \"ما المستوى الذي تفضله لدمج أدوات الذكاء الاصطناعي في تجربتك التعليمية؟\": \"Q33\",\n",
    "    \"هل لديك أي اقتراحات أو ملاحظات حول استخدام الذكاء الاصطناعي في العملية التعليمية، بما في ذلك الجوانب التي تحتاج إلى تحسين؟\": \"Q34\"\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Print cleaned column names for verification\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce3a2f-ea2f-46c5-a414-6bc653f99558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create a group label\n",
    "def get_group(row):\n",
    "    level = row['Q2']\n",
    "    language = row['Q3']\n",
    "    return f\"{level}_{language}\"\n",
    "\n",
    "df['group'] = df.apply(get_group, axis=1)\n",
    "\n",
    "print(df['group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0313c11-0ceb-45ef-829e-baf7e7d42031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction to the target size\n",
    "# Define the target sizes for each group\n",
    "target_counts = {\n",
    "    \"السنة الأولى_العربية\": 190,\n",
    "    \"السنة الرابعة_العربية\": 150,\n",
    "    \"السنة الرابعة_الإنجليزية أو الفرنسية\": 150,\n",
    "    \"السنة الأولى_الإنجليزية أو الفرنسية\": 110\n",
    "}\n",
    "\n",
    "# Create an empty list to hold the reduced DataFrame pieces\n",
    "dfs = []\n",
    "\n",
    "# Loop through each group and apply sampling\n",
    "for group_name, target_size in target_counts.items():\n",
    "    group_df = df[df['group'] == group_name]\n",
    "    \n",
    "    # If group has more rows than target, sample down\n",
    "    if len(group_df) > target_size:\n",
    "        group_df = group_df.sample(n=target_size, random_state=42)\n",
    "    \n",
    "    dfs.append(group_df)\n",
    "\n",
    "# Combine all sampled groups into a single DataFrame\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Check the new counts\n",
    "print(df['group'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d75960-46d5-425d-ba05-5149ed91c00d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40959459-4921-4bef-a9ed-4a0022f541b3",
   "metadata": {},
   "source": [
    "#### Coding the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f767581-ea9a-4ca2-a5fd-f636c77070af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Binary variables\n",
    "binary_map = {\n",
    "    'Q1': {'ذكر': 1, 'أنثى': 0},\n",
    "    'Q7': {'نعم': 1, 'لا': 0},\n",
    "    'Q29': {'نعم': 1, 'لا': 0}\n",
    "}\n",
    "for col, mapping in binary_map.items():\n",
    "    df[col] = df[col].map(mapping)\n",
    "\n",
    "\n",
    "# 2. Nominal categorical variables (manual mapping)\n",
    "q2_map = {\n",
    "    'السنة الأولى': 0,\n",
    "    'السنة الرابعة': 1\n",
    "}\n",
    "df['Q2'] = df['Q2'].map(q2_map)\n",
    "\n",
    "q3_map = {\n",
    "    'العربية': 0,\n",
    "    'الإنجليزية أو الفرنسية': 1\n",
    "}\n",
    "df['Q3'] = df['Q3'].map(q3_map)\n",
    "\n",
    "q4_map = {\n",
    "    'لم أتخصص بعد': 0,\n",
    "    'العلوم السياسية': 1,\n",
    "    'الاقتصاد': 2,\n",
    "    'الإحصاء': 3,\n",
    "}\n",
    "df['Q4'] = df['Q4'].map(q4_map)\n",
    "\n",
    "group_map = {\n",
    "    'السنة الأولى_العربية': 1,\n",
    "    'السنة الأولى_الإنجليزية أو الفرنسية': 2,\n",
    "    'السنة الرابعة_العربية': 3,\n",
    "    'السنة الرابعة_الإنجليزية أو الفرنسية': 4\n",
    "}\n",
    "df['group'] = df['group'].map(group_map)\n",
    "\n",
    "\n",
    "# 3. Ordinal variables\n",
    "# GPA Range\n",
    "gpa_map = {\n",
    "    'أقل من 2.6': 0,\n",
    "    'من 2.6 إلى أقل من 3.6': 1,\n",
    "    'من 3.6 إلى 4.0': 2\n",
    "}\n",
    "df['Q5'] = df['Q5'].map(gpa_map)\n",
    "\n",
    "# Hours per day\n",
    "hours_map = {\n",
    "    'أقل من ساعة': 1,\n",
    "    'من ساعة إلى أقل من 3 ساعات': 2,\n",
    "    'من 3 ساعات إلى أقل من 6 ساعات': 3,\n",
    "    'من 6 ساعات إلى أقل من 9 ساعات': 4,\n",
    "    '9 ساعات أو أكثر': 5\n",
    "}\n",
    "df['Q6'] = df['Q6'].map(hours_map)\n",
    "\n",
    "# Ordinal mapping for Q11\n",
    "q11_map = {\n",
    "    'أقل من 3 أشهر': 0,\n",
    "    'من 3 أشهر إلى أقل من سنة': 1,\n",
    "    'سنة أو أكثر': 2\n",
    "}\n",
    "df['Q11'] = df['Q11'].map(q11_map)\n",
    "\n",
    "# Ordinal mapping for Q12\n",
    "q12_map = {\n",
    "    'نادرًا – مرة واحدة شهريًا أو أقل': 1,\n",
    "    'أحيانًا – مرتين إلى ثلاث مرات شهريًا': 2,\n",
    "    'بشكل معتدل – أسبوعيًا': 3,\n",
    "    'بشكل متكرر – عدة مرات أسبوعيًا': 4,\n",
    "    'بشكل يومي – جزء أساسي من دراستي': 5\n",
    "}\n",
    "df['Q12'] = df['Q12'].map(q12_map)\n",
    "\n",
    "\n",
    "# 4. Convert Likert scale to numeric:\n",
    "likert_cols = (\n",
    "    [f'Q{i}' for i in range(9, 11)] +          # Q9 and Q10 only\n",
    "    [f'Q13_{i}' for i in range(1, 8)] +        # Q13_1 to Q13_7\n",
    "    [f'Q{i}' for i in range(14, 29)] +         # Q14 to Q28\n",
    "    ['Q30', 'Q33']                             # Q30 and Q33\n",
    ")\n",
    "\n",
    "# Convert Likert-scale columns to numeric\n",
    "df[likert_cols] = df[likert_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf31f4b-3480-4780-9f41-489ab0e2943d",
   "metadata": {},
   "source": [
    "### Splitting the data frame into who used AI in education and who did not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab620105-1c2d-48a8-867a-4ce35297f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those who continued the survey\n",
    "df_yes = df[df['Q7'] == 1].copy()\n",
    "\n",
    "# Those who didn't\n",
    "df_no = df[df['Q7'] == 0].copy()\n",
    "\n",
    "print(\"Count of respondents who use AI in education:\", df_yes.shape[0])\n",
    "print(\"Count of respondents who do not use AI in education:\", df_no.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62881a-949f-4906-9830-094d57959263",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 2. Data Description\n",
    "Below are the steps that were used to do the data description:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55b6c5-1088-48ba-ad3d-515d59cfa7ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b09ad-2781-4809-8841-5f8e9e04e211",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b89088-1085-46ae-9842-03ab883af8fe",
   "metadata": {},
   "source": [
    "### Demographic Profile of Respondents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b868f-6c5b-4a8f-9ba7-ca1ff5015b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the combined categorical variable\n",
    "df['Gender_Language'] = df['Q1'].map({1: 'Male', 0: 'Female'}) + ' – ' + df['Q3'].map({0: 'Arabic', 1: 'English/French'})\n",
    "\n",
    "# Count combinations\n",
    "combined_counts = df['Gender_Language'].value_counts()\n",
    "\n",
    "# Define custom order for clockwise layout: Female–Arabic, Female–English, Male–English, Male–Arabic\n",
    "ordered_labels = ['Female – Arabic', 'Female – English/French', 'Male – English/French', 'Male – Arabic']\n",
    "counts = [combined_counts.get(label, 0) for label in ordered_labels]\n",
    "\n",
    "# Define matching colors: pinks for females, blues for males\n",
    "colors = ['#f4a6b7', '#f7c6d3', '#a3c7f7', '#72b3f0']\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(counts, labels=ordered_labels, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "\n",
    "# Title and layout\n",
    "plt.title('Combined Distribution of Gender and Study Language', pad=10)\n",
    "plt.axis('equal')  # Keep pie circular\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15dcb20-28e6-4a2b-b8f3-6d04482084fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out undeclared majors (value 0)\n",
    "filtered_majors = df[df['Q4'] != 0]\n",
    "\n",
    "# Count major responses and map labels\n",
    "major_counts = filtered_majors['Q4'].value_counts().sort_index()\n",
    "major_labels = {1: 'Political Science', 2: 'Economics', 3: 'Statistics'}\n",
    "major_names = [major_labels[i] for i in major_counts.index]\n",
    "major_percentages = (major_counts / major_counts.sum()) * 100\n",
    "\n",
    "# GPA counts and percentages\n",
    "gpa_counts = df['Q5'].value_counts().sort_index()\n",
    "gpa_labels = ['Less than 2.6', '2.6 to less than 3.6', '3.6 to 4.0']\n",
    "gpa_percentages = (gpa_counts / gpa_counts.sum()) * 100\n",
    "\n",
    "# Create bar charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# GPA Bar Chart\n",
    "axes[0].bar(gpa_labels, gpa_percentages, color='#66b3ff')\n",
    "axes[0].set_title(\"GPA Distribution\", pad=10)\n",
    "axes[0].set_ylabel(\"Percentage of Respondents\")\n",
    "axes[0].tick_params(axis='x', rotation=20)\n",
    "for i, val in enumerate(gpa_percentages):\n",
    "    axes[0].text(i, val + 1, f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Major Bar Chart\n",
    "axes[1].bar(major_names, major_percentages, color='#99ff99')\n",
    "axes[1].set_title(\"Major Distribution\", pad=10)\n",
    "axes[1].set_ylabel(\"Percentage of Respondents\")\n",
    "axes[1].tick_params(axis='x', rotation=20)\n",
    "for i, val in enumerate(major_percentages):\n",
    "    axes[1].text(i, val + 1, f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a9c78-105e-4f9f-b0cd-d3c34ba68ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define labels\n",
    "gpa_labels = {0: '< 2.6', 1: '2.6 – <3.6', 2: '3.6 – 4.0'}\n",
    "level_labels = {0: 'First Year', 1: 'Fourth Year'}\n",
    "lang_labels = {0: 'Arabic', 1: 'English/French'}\n",
    "major_labels = {1: 'Political Science', 2: 'Economics', 3: 'Statistics'}\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Function to plot with counts on Y-axis, percentage labels inside, and total on top\n",
    "def plot_stacked_bar_with_percent_labels(ax, data, title, legend_title, colors):\n",
    "    totals = data.sum(axis=1)\n",
    "    bottom = [0] * len(data)\n",
    "\n",
    "    for i, col in enumerate(data.columns):\n",
    "        values = data[col]\n",
    "        bar = ax.bar(data.index, values, bottom=bottom, label=col, color=colors[i])\n",
    "        for j, (val, total) in enumerate(zip(values, totals)):\n",
    "            if val > 0:\n",
    "                percent = val / total * 100\n",
    "                ax.text(j, bottom[j] + val / 2, f'{percent:.1f}%', ha='center', va='center', fontsize=9, color='white')\n",
    "        bottom = [bottom[k] + values.iloc[k] for k in range(len(data))]\n",
    "\n",
    "    # Total count on top of each bar\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(i, total + max(totals)*0.02, str(total), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('GPA Range')\n",
    "    ax.set_ylabel('Number of Respondents')\n",
    "    ax.legend(title=legend_title)\n",
    "    ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# GPA vs Academic Level\n",
    "gpa_level = pd.crosstab(df['Q5'], df['Q2'])\n",
    "gpa_level.index = [gpa_labels[i] for i in gpa_level.index]\n",
    "gpa_level.columns = [level_labels[i] for i in gpa_level.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[0], gpa_level, 'GPA Distribution by Academic Level', 'Academic Level', ['#1f77b4', '#ff7f0e'])\n",
    "\n",
    "# GPA vs Study Language\n",
    "gpa_lang = pd.crosstab(df['Q5'], df['Q3'])\n",
    "gpa_lang.index = [gpa_labels[i] for i in gpa_lang.index]\n",
    "gpa_lang.columns = [lang_labels[i] for i in gpa_lang.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[1], gpa_lang, 'GPA Distribution by Study Language', 'Study Language', ['#1f77b4', '#ff7f0e'])\n",
    "\n",
    "# GPA vs Major\n",
    "df_major_filtered = df[df['Q4'] != 0]\n",
    "gpa_major = pd.crosstab(df_major_filtered['Q5'], df_major_filtered['Q4'])\n",
    "gpa_major.index = [gpa_labels[i] for i in gpa_major.index]\n",
    "gpa_major.columns = [major_labels[i] for i in gpa_major.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[2], gpa_major, 'GPA Distribution by Major', 'Major', ['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db83e4c-e84c-49de-947c-4f2d39800a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate percentages\n",
    "tech_counts = df['Q6'].value_counts(normalize=True).sort_index() * 100\n",
    "tech_labels = {\n",
    "    1: '< 1 hour',\n",
    "    2: '1 – <3 hours',\n",
    "    3: '3 – <6 hours',\n",
    "    4: '6 – <9 hours',\n",
    "    5: '9+ hours'\n",
    "}\n",
    "\n",
    "# Plot vertical bar chart\n",
    "plt.figure(figsize=(7, 4))\n",
    "bars = sns.barplot(\n",
    "    x=[tech_labels[i] for i in tech_counts.index],\n",
    "    y=tech_counts.values\n",
    ")\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, bar in enumerate(bars.patches):\n",
    "    height = bar.get_height()\n",
    "    bars.text(\n",
    "        bar.get_x() + bar.get_width()/2.,\n",
    "        height + 1,\n",
    "        f'{height:.1f}%',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"Daily Hours of Technology Use\")\n",
    "plt.xlabel(\"Daily Usage\")\n",
    "plt.ylabel(\"Percentage of Respondents\")\n",
    "plt.ylim(0, max(tech_counts.values) + 10)  # Add room for labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d74c8-2be9-409d-8749-0f1fe896a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# English labels for mapped Q6 values\n",
    "tech_labels = {\n",
    "    1: '<1 hour',\n",
    "    2: '1 to <3 hours',\n",
    "    3: '3 to <6 hours',\n",
    "    4: '6 to <9 hours',\n",
    "    5: '9 hours or more'\n",
    "}\n",
    "\n",
    "level_labels = {0: 'First Year', 1: 'Fourth Year'}\n",
    "lang_labels = {0: 'Arabic', 1: 'English/French'}\n",
    "gpa_labels = {0: '< 2.6', 1: '2.6 – <3.6', 2: '3.6 – 4.0'}\n",
    "\n",
    "# Plot function (same logic as before)\n",
    "def plot_stacked_bar_with_percent_labels(ax, data, title, legend_title, colors):\n",
    "    totals = data.sum(axis=1)\n",
    "    bottom = [0] * len(data)\n",
    "    for i, col in enumerate(data.columns):\n",
    "        values = data[col]\n",
    "        bar = ax.bar(data.index, values, bottom=bottom, label=col, color=colors[i % len(colors)])\n",
    "        for j, (val, total) in enumerate(zip(values, totals)):\n",
    "            if val > 0:\n",
    "                percent = val / total * 100\n",
    "                ax.text(j, bottom[j] + val / 2, f'{percent:.1f}%', ha='center', va='center', fontsize=9, color='white')\n",
    "        bottom = [bottom[k] + values.iloc[k] for k in range(len(data))]\n",
    "    \n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(i, total + max(totals)*0.02, str(total), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Daily Tech Usage')\n",
    "    ax.set_ylabel('Number of Respondents')\n",
    "    ax.legend(title=legend_title)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "color_set_2 = ['#1f77b4', '#ff7f0e']\n",
    "color_set_3 = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Tech Usage vs Academic Level\n",
    "tech_level_crosstab = pd.crosstab(df['Q6'], df['Q2'])\n",
    "tech_level_crosstab.index = [tech_labels[i] for i in tech_level_crosstab.index]\n",
    "tech_level_crosstab.columns = [level_labels[i] for i in tech_level_crosstab.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[0], tech_level_crosstab, 'Technology Usage by Academic Level', 'Academic Level', color_set_2)\n",
    "\n",
    "# Tech Usage vs Study Language\n",
    "tech_lang_crosstab = pd.crosstab(df['Q6'], df['Q3'])\n",
    "tech_lang_crosstab.index = [tech_labels[i] for i in tech_lang_crosstab.index]\n",
    "tech_lang_crosstab.columns = [lang_labels[i] for i in tech_lang_crosstab.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[1], tech_lang_crosstab, 'Technology Usage by Study Language', 'Study Language', color_set_2)\n",
    "\n",
    "# Tech Usage vs GPA Level\n",
    "tech_gpa_crosstab = pd.crosstab(df['Q6'], df['Q5'])\n",
    "tech_gpa_crosstab.index = [tech_labels[i] for i in tech_gpa_crosstab.index]\n",
    "tech_gpa_crosstab.columns = [gpa_labels[i] for i in tech_gpa_crosstab.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[2], tech_gpa_crosstab, 'Technology Usage by GPA Level', 'GPA Level', color_set_3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ea044-6683-40e9-9ee0-87064a1b2ff2",
   "metadata": {},
   "source": [
    "### Usage of Generative AI Tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8111fd4-6782-4cf3-bfd1-b036ec18e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Labels\n",
    "ai_labels = {1: 'Yes', 0: 'No'}\n",
    "gpa_labels = {0: '< 2.6', 1: '2.6 – <3.6', 2: '3.6 – 4.0'}\n",
    "level_labels = {0: 'First Year', 1: 'Fourth Year'}\n",
    "\n",
    "# Plot function for stacked bars with percentages inside and totals on top\n",
    "def plot_stacked_bar_with_percent_labels(ax, data, title, legend_title, colors):\n",
    "    totals = data.sum(axis=1)\n",
    "    bottom = [0] * len(data)\n",
    "    \n",
    "    for i, col in enumerate(data.columns):\n",
    "        values = data[col]\n",
    "        bar = ax.bar(data.index, values, bottom=bottom, label=col, color=colors[i % len(colors)])\n",
    "        \n",
    "        # Add percentage labels inside bars\n",
    "        for j, (val, total) in enumerate(zip(values, totals)):\n",
    "            if val > 0:\n",
    "                percent = val / total * 100\n",
    "                ax.text(j, bottom[j] + val / 2, f'{percent:.1f}%', ha='center', va='center', \n",
    "                       fontsize=9, color='white', fontweight='bold')\n",
    "        \n",
    "        bottom = [bottom[k] + values.iloc[k] for k in range(len(data))]\n",
    "    \n",
    "\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Number of Respondents')\n",
    "    ax.legend(title=legend_title)\n",
    "    ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Create AI usage percentage bar chart data\n",
    "ai_counts = df['Q7'].map(ai_labels).value_counts().sort_index()\n",
    "ai_percentages = (ai_counts / ai_counts.sum() * 100).round(1)\n",
    "\n",
    "# Plot all 3 charts side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# --- Chart 1: Overall AI Usage (Simple Bar) ---\n",
    "bars = axes[0].bar(ai_percentages.index, ai_percentages.values, color=['#1f77b4', '#ff7f0e'])\n",
    "\n",
    "# Add percentage labels on top\n",
    "for bar, percent in zip(bars, ai_percentages.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{percent}%', \n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "axes[0].set_title('Overall AI Tool Usage')\n",
    "axes[0].set_ylabel('Percentage of Students')\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "# --- Chart 2: AI Usage vs GPA Level ---\n",
    "ai_gpa_crosstab = pd.crosstab(df['Q7'], df['Q5'])\n",
    "ai_gpa_crosstab.index = [ai_labels[i] for i in ai_gpa_crosstab.index]\n",
    "ai_gpa_crosstab.columns = [gpa_labels[i] for i in ai_gpa_crosstab.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[1], ai_gpa_crosstab, 'AI Tool Usage by GPA Level', 'GPA Level', ['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1].set_xlabel('AI Tool Usage')\n",
    "\n",
    "# --- Chart 3: AI Usage vs Academic Level ---\n",
    "ai_level_crosstab = pd.crosstab(df['Q7'], df['Q2'])\n",
    "ai_level_crosstab.index = [ai_labels[i] for i in ai_level_crosstab.index]\n",
    "ai_level_crosstab.columns = [level_labels[i] for i in ai_level_crosstab.columns]\n",
    "plot_stacked_bar_with_percent_labels(axes[2], ai_level_crosstab, 'AI Tool Usage by Academic Level', 'Academic Level', ['#1f77b4', '#ff7f0e'])\n",
    "axes[2].set_xlabel('AI Tool Usage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b510f-e932-4abd-8814-0f3917eb3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Safely normalize the column\n",
    "tool_column = df['Q8'].dropna().str.lower().str.strip()\n",
    "\n",
    "# Categorize into groups (remove Claude category, let it fall into Others)\n",
    "normalized_tools = pd.Series('Others', index=df.index)\n",
    "normalized_tools.loc[tool_column[tool_column.str.contains('chatgpt|gpt', na=False)].index] = 'ChatGPT'\n",
    "normalized_tools.loc[tool_column[tool_column.str.contains('gemini|bard', na=False)].index] = 'Gemini'\n",
    "normalized_tools.loc[tool_column[tool_column.str.contains('deepseek', na=False)].index] = 'DeepSeek'\n",
    "# Claude entries will automatically fall into 'Others' since we removed its specific mapping\n",
    "\n",
    "# Add column to DataFrame\n",
    "df['AI_Tool_Category'] = normalized_tools\n",
    "\n",
    "# Count the grouped results for the bar chart\n",
    "tool_counts = df['AI_Tool_Category'].value_counts()\n",
    "tool_counts = tool_counts.reindex(['ChatGPT', 'Gemini', 'DeepSeek', 'Others']).fillna(0).astype(int)\n",
    "\n",
    "# Calculate percentages for the bar chart\n",
    "total_students = tool_counts.sum()\n",
    "tool_percentages = (tool_counts / total_students * 100).round(1)\n",
    "\n",
    "# Plot the bar chart with percentages\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(tool_percentages.index, tool_percentages.values, color=['#1f77b4', '#ff7f0e', '#9467bd', '#7f7f7f'])\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "plt.title('Usage of Popular AI Tools Among Students')\n",
    "plt.ylabel('Percentage of Students (%)')\n",
    "plt.xlabel('AI Tool')\n",
    "plt.ylim(0, 50)  # Set y-axis limit to 0-50% for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create the frequency table\n",
    "freq_table = df['AI_Tool_Category'].value_counts().reset_index()\n",
    "freq_table.columns = ['AI Tool', 'Number of Students']\n",
    "freq_table['Percentage'] = (freq_table['Number of Students'] / freq_table['Number of Students'].sum()) * 100\n",
    "freq_table['Percentage'] = freq_table['Percentage'].round(1)\n",
    "\n",
    "# Reorder the table to match the desired order\n",
    "freq_table = freq_table.set_index('AI Tool').reindex(['ChatGPT', 'DeepSeek', 'Gemini', 'Others']).reset_index()\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nTable 3.9: Frequency of Most Used AI Tools Among Students\")\n",
    "print(\"-\" * 50)\n",
    "print(freq_table.to_string(index=False))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc85fe-7f49-4b7b-bd05-47675a770f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values from Q29\n",
    "q29_clean = df['Q29'].dropna()\n",
    "\n",
    "# Count how many students pay vs don't pay\n",
    "counts = q29_clean.value_counts().sort_index()\n",
    "\n",
    "# Labels for the chart\n",
    "labels = ['Do Not Pay', 'Pay']\n",
    "colors = ['#FF6B6B', '#4ECDC4']  # Modern, vibrant colors\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "wedges, texts, autotexts = plt.pie(counts, labels=labels, autopct='%1.1f%%', \n",
    "                                  startangle=145, colors=colors)\n",
    "\n",
    "# Make all text bold\n",
    "for text in texts:\n",
    "    text.set_fontweight('bold')\n",
    "    text.set_fontsize(12)\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontweight('bold') \n",
    "    autotext.set_fontsize(12)\n",
    "\n",
    "plt.title('Do Students Pay for AI Tools?', fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb194f-bce2-466c-9933-27817d5ec9b1",
   "metadata": {},
   "source": [
    "### Describing the research questions (Likert scale):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd3d85-7650-4558-bd8f-e54695d44c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of Q13 sub-questions\n",
    "q13_columns = [f'Q13_{i}' for i in range(1, 8)]\n",
    "\n",
    "# Make sure values are numeric\n",
    "df[q13_columns] = df[q13_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Create empty DataFrame for combined results\n",
    "combined_freq = pd.DataFrame(index=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Loop through each question and calculate frequencies\n",
    "for col in q13_columns:\n",
    "    freq = df[col].value_counts().reindex([1, 2, 3, 4, 5], fill_value=0)\n",
    "    combined_freq[col] = freq\n",
    "\n",
    "# Rename index for clarity\n",
    "combined_freq.index.name = 'Response (1–5)'\n",
    "\n",
    "# Optional: Rename columns to more descriptive labels\n",
    "question_labels = {\n",
    "    'Q13_1': 'Idea Generation',\n",
    "    'Q13_2': 'Content Creation',\n",
    "    'Q13_3': 'Academic Writing',\n",
    "    'Q13_4': 'Presentation Support',\n",
    "    'Q13_5': 'Translation',\n",
    "    'Q13_6': 'Programming',\n",
    "    'Q13_7': 'Exam Preparation'\n",
    "}\n",
    "combined_freq = combined_freq.rename(columns=question_labels)\n",
    "\n",
    "# Display the final frequency table\n",
    "print(\"Combined Frequency Table (Q13.1 to Q13.7)\")\n",
    "print(combined_freq)\n",
    "\n",
    "# Combined Percentage Table (Q13)\n",
    "combined_pct = pd.DataFrame(index=[1, 2, 3, 4, 5])\n",
    "\n",
    "for col in q13_columns:\n",
    "    freq = df[col].value_counts(normalize=True).reindex([1, 2, 3, 4, 5], fill_value=0) * 100\n",
    "    combined_pct[col] = freq.round(1)\n",
    "\n",
    "combined_pct.index.name = 'Response (1–5)'\n",
    "combined_pct = combined_pct.rename(columns=question_labels)\n",
    "\n",
    "print(\"Combined Percentage Table (Q13.1 to Q13.7)\")\n",
    "print(combined_pct)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Define all Q13 sub-questions\n",
    "q13_columns = [f'Q13_{i}' for i in range(1, 8)]\n",
    "level_labels = {0: 'First Year', 1: 'Fourth Year'}\n",
    "\n",
    "# Create a dictionary to store results\n",
    "significance_results = []\n",
    "\n",
    "# Loop through each question\n",
    "for col in q13_columns:\n",
    "    # Ensure numeric\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Drop missing data\n",
    "    temp_df = df[[col, 'Q2']].dropna()\n",
    "    \n",
    "    # Crosstab (frequency table)\n",
    "    freq_table = pd.crosstab(temp_df[col], temp_df['Q2'])\n",
    "    \n",
    "    # Chi-square test of independence\n",
    "    try:\n",
    "        chi2, p, dof, expected = chi2_contingency(freq_table)\n",
    "        significance_results.append({\n",
    "            'Question': col,\n",
    "            'Chi2': chi2,\n",
    "            'p-value': p,\n",
    "            'Significant': 'Yes' if p < 0.05 else 'No'\n",
    "        })\n",
    "        \n",
    "        # Print frequency table\n",
    "        print(f\"\\nFrequency Table for {col}\")\n",
    "        print(freq_table)\n",
    "        print(f\"Chi-square p-value: {p:.4f} ({'Significant' if p < 0.05 else 'Not Significant'})\")\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"\\nSkipped {col} due to error: {e}\")\n",
    "\n",
    "# Convert summary to DataFrame and display\n",
    "summary_df = pd.DataFrame(significance_results)\n",
    "print(\"\\nSUMMARY OF CHI-SQUARE TESTS\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Relevant columns\n",
    "significant_columns = ['Q13_3', 'Q13_6']  # Academic Writing, Programming\n",
    "question_labels = {\n",
    "    'Q13_3': 'Academic Writing',\n",
    "    'Q13_6': 'Programming'\n",
    "}\n",
    "level_labels = {0: 'First Year', 1: 'Fourth Year'}\n",
    "\n",
    "# Likert scale axis labels\n",
    "likert_labels = {\n",
    "    1: '1 = Never',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5 = Always'\n",
    "}\n",
    "\n",
    "# Ensure columns are numeric\n",
    "for col in significant_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "for i, col in enumerate(significant_columns):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Filter valid data\n",
    "    temp_df = df[[col, 'Q2']].dropna()\n",
    "    \n",
    "    # Crosstab: raw counts\n",
    "    ctab = pd.crosstab(temp_df[col], temp_df['Q2'])\n",
    "    ctab = ctab.reindex([1, 2, 3, 4, 5], fill_value=0)\n",
    "    ctab.columns = [level_labels.get(x, str(x)) for x in ctab.columns]\n",
    "    \n",
    "    # Plot\n",
    "    ctab.plot(kind='bar', ax=ax, edgecolor='black')\n",
    "    ax.set_title(question_labels[col])\n",
    "    ax.set_xlabel(\"Usage Frequency (1 = Never, 5 = Always)\")\n",
    "    ax.set_ylabel(\"Number of Students\")\n",
    "    ax.set_ylim(0, ctab.values.max() + 10)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels([likert_labels[i] for i in range(1, 6)])\n",
    "    \n",
    "    # Add counts\n",
    "    #for container in ax.containers:\n",
    "    #    ax.bar_label(container, fmt='%d', fontsize=9, padding=2)\n",
    "\n",
    "plt.suptitle(\"The Significant AI Usage Purposes by Academic Level\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Reduced top margin from 0.92 to 0.95\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6929c42-8098-4e89-98a3-4519e2357584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a3821-83b3-4411-9a7c-c94672dff3d3",
   "metadata": {},
   "source": [
    "## 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8f680-ff90-4f5d-9c98-1f53f77dec81",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb9925-29a7-4a97-b6c4-050a419b38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of questions in the impact section\n",
    "impact_questions = [f'Q{i}' for i in range(14, 25)]\n",
    "\n",
    "# Stack all answers from Q14–Q24 into one Series\n",
    "all_responses = df[impact_questions].stack().dropna()\n",
    "\n",
    "# Count frequencies of values 1 to 5\n",
    "response_counts = all_responses.value_counts().sort_index()\n",
    "\n",
    "# Convert to percentage\n",
    "response_percentages = (response_counts / response_counts.sum()) * 100\n",
    "\n",
    "# Ensure all 1–5 values are present\n",
    "for i in range(1, 6):\n",
    "    if i not in response_percentages:\n",
    "        response_percentages[i] = 0\n",
    "\n",
    "# Sort by Likert scale order\n",
    "response_percentages = response_percentages.sort_index()\n",
    "\n",
    "# Plot with enhanced styling\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#87CEEB', '#87CEEB', '#87CEEB', '#87CEEB', '#87CEEB']  # Gradient colors\n",
    "bars = plt.bar(response_percentages.index, response_percentages.values, \n",
    "               color=colors, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "# Add percentage labels inside bars (no need for high percentages above)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2,  # X position (middle of the bar)\n",
    "             height / 2,                          # Y position (middle of the bar)\n",
    "             f'{height:.1f}%',                    # Label text\n",
    "             ha='center',                         # Horizontally center the label\n",
    "             va='center',                         # Vertically center the label\n",
    "             fontsize=11,\n",
    "             fontweight='bold',\n",
    "             color='white')\n",
    "\n",
    "plt.ylabel('Percentage of Responses (%)', fontsize=12)\n",
    "plt.xlabel('Response Value (1 = Lowest, 5 = Highest)', fontsize=12)\n",
    "plt.title('Distribution of Responses (Q14–Q24 Combined)', fontsize=14, pad=20)\n",
    "\n",
    "# Enhanced grid and styling\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3, color='gray')\n",
    "plt.yticks(range(0, 35, 5))  # Adjusted range to fit the data better\n",
    "plt.xticks(response_percentages.index)\n",
    "\n",
    "# Add a subtle background color\n",
    "plt.gca().set_facecolor('#F8F9FA')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f8c93-2099-488d-9bf0-fee98fa070bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define question labels\n",
    "impact_questions = [f'Q{i}' for i in range(14, 25)]\n",
    "impact_labels = {\n",
    "    'Q14': 'Effect on GPA',\n",
    "    'Q15': 'Assignment Quality',\n",
    "    'Q16': 'Research Skills',\n",
    "    'Q17': 'Use for Study/Exams',\n",
    "    'Q18': 'Professors’ Support',\n",
    "    'Q19': 'Time-Saving',\n",
    "    'Q20': 'Critical Thinking',\n",
    "    'Q21': 'Academic Writing',\n",
    "    'Q22': 'Self-Learning',\n",
    "    'Q23': 'Teamwork Skills',\n",
    "    'Q24': 'Academic Satisfaction'\n",
    "}\n",
    "\n",
    "# Subset the data and drop all-NaN rows in the subset\n",
    "data = df[impact_questions].dropna(how='all')\n",
    "\n",
    "# Compute Spearman correlation and p-values\n",
    "corr_matrix, pval_matrix = spearmanr(data, nan_policy='omit')\n",
    "\n",
    "# Create DataFrames\n",
    "corr_df = pd.DataFrame(corr_matrix, index=impact_questions, columns=impact_questions)\n",
    "pval_df = pd.DataFrame(pval_matrix, index=impact_questions, columns=impact_questions)\n",
    "\n",
    "# Rename indices/columns for plotting\n",
    "corr_df.rename(index=impact_labels, columns=impact_labels, inplace=True)\n",
    "pval_df.rename(index=impact_labels, columns=impact_labels, inplace=True)\n",
    "\n",
    "# Format annotations: \"corr\\n(p=pval)\"\n",
    "annot = corr_df.round(2).astype(str) + \"\\n(p=\" + pval_df.round(3).astype(str) + \")\"\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_df, \n",
    "    annot=annot, \n",
    "    fmt='', \n",
    "    cmap='coolwarm', \n",
    "    center=0, \n",
    "    square=True, \n",
    "    linewidths=.5, \n",
    "    cbar_kws={'label': 'Spearman Correlation'}\n",
    ")\n",
    "\n",
    "plt.title(\"Spearman Correlation Heatmap with P-values\\n(Q14–Q24: Academic Impact)\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0a967-604c-49a3-a9f9-d6e5c6053614",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd302c-0351-4827-b8f0-cc75ab7e3b16",
   "metadata": {},
   "source": [
    "### 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e471a2c-a698-4aca-a7b4-a916a3018850",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547f80f-ede2-44ad-b0d2-c82bdc15c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Challenge questions and titles\n",
    "challenge_questions = {\n",
    "    'Q25': 'Delays in Results',\n",
    "    'Q26': 'Concern about Misinformation',\n",
    "    'Q27': 'System Compatibility Issues',\n",
    "    'Q28': 'Ethical Concerns (Privacy, Security)'\n",
    "}\n",
    "\n",
    "# Initialize empty dataframe\n",
    "combined_table = pd.DataFrame({'Rating': [1, 2, 3, 4, 5]})\n",
    "\n",
    "for q, title in challenge_questions.items():\n",
    "    # Clean data\n",
    "    counts = df[q].dropna().value_counts().reindex([1, 2, 3, 4, 5], fill_value=0)\n",
    "    percentages = (counts / counts.sum() * 100).round(1)\n",
    "\n",
    "    # Combine count and percentage into one column\n",
    "    combined = [f\"{counts[i]} ({percentages[i]}%)\" for i in [1, 2, 3, 4, 5]]\n",
    "    combined_table[title] = combined\n",
    "\n",
    "# Optional: format the table for output\n",
    "print(\"Table 3.2: Frequency and Percentage Distribution of AI-Related Challenge Responses (Q25–Q28)\")\n",
    "print(combined_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44864c9-74cb-4d21-b2ca-efdd2b646bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 Questions\n",
    "section4_questions = [f'Q{i}' for i in range(25, 29)]\n",
    "\n",
    "# Stack all responses from Q25–Q28 into a single Series\n",
    "all_responses = df[section4_questions].stack().dropna()\n",
    "\n",
    "# Count the values from 1 to 5\n",
    "response_counts = all_responses.value_counts().sort_index()\n",
    "\n",
    "# Convert to percentage\n",
    "response_percentages = (response_counts / response_counts.sum()) * 100\n",
    "\n",
    "# Ensure values 1–5 are present\n",
    "for i in range(1, 6):\n",
    "    if i not in response_percentages:\n",
    "        response_percentages[i] = 0\n",
    "\n",
    "# Sort\n",
    "response_percentages = response_percentages.sort_index()\n",
    "\n",
    "# Plot with enhanced styling\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#90EE90'] * 5  # Light green color for all bars\n",
    "bars = plt.bar(response_percentages.index, response_percentages.values,\n",
    "               color=colors, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "# Add percentage labels inside bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2,\n",
    "             height / 2,\n",
    "             f'{height:.1f}%',\n",
    "             ha='center',\n",
    "             va='center',\n",
    "             fontsize=11,\n",
    "             fontweight='bold',\n",
    "             color='black')\n",
    "\n",
    "plt.ylabel('Percentage of Responses (%)', fontsize=12)\n",
    "plt.xlabel('Response Value (1 = Lowest, 5 = Highest)', fontsize=12)\n",
    "plt.title('Distribution of Responses (Q25–Q28 Combined)', fontsize=14, pad=20)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3, color='gray')\n",
    "plt.yticks(range(0, 35, 5))\n",
    "plt.xticks(response_percentages.index)\n",
    "\n",
    "plt.gca().set_facecolor('#F8F9FA')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802db3d-a11c-44e0-ae75-6317f927f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define section 4 questions and their labels\n",
    "section4_questions = [f'Q{i}' for i in range(25, 29)]\n",
    "section4_labels = {\n",
    "    'Q25': 'Ethical Concerns',\n",
    "    'Q26': 'Academic Integrity',\n",
    "    'Q27': 'AI Misuse Concerns',\n",
    "    'Q28': 'Need for Regulation'\n",
    "}\n",
    "\n",
    "# Drop rows where all are NaN\n",
    "data = df[section4_questions].dropna(how='all')\n",
    "\n",
    "# Spearman correlation and p-values\n",
    "corr_matrix, pval_matrix = spearmanr(data, nan_policy='omit')\n",
    "\n",
    "# Convert to DataFrames\n",
    "corr_df = pd.DataFrame(corr_matrix, index=section4_questions, columns=section4_questions)\n",
    "pval_df = pd.DataFrame(pval_matrix, index=section4_questions, columns=section4_questions)\n",
    "\n",
    "# Rename for display\n",
    "corr_df.rename(index=section4_labels, columns=section4_labels, inplace=True)\n",
    "pval_df.rename(index=section4_labels, columns=section4_labels, inplace=True)\n",
    "\n",
    "# Combine annotations\n",
    "annot = corr_df.round(2).astype(str) + \"\\n(p=\" + pval_df.round(3).astype(str) + \")\"\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    corr_df,\n",
    "    annot=annot,\n",
    "    fmt='',\n",
    "    cmap='YlOrBr',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Spearman Correlation'}\n",
    ")\n",
    "\n",
    "plt.title(\"Spearman Correlation Heatmap with P-values\\n(Q25–Q28: Perceptions of AI Ethics)\", fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22d0a8-6925-4a15-bc7a-60132327bef3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf5682-21bb-4a81-a853-758b65f2f8b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd05112-b0f7-474f-840e-d361d6be14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Q2 and Q33 are numeric and drop missing values\n",
    "df['Q2'] = pd.to_numeric(df['Q2'], errors='coerce')  # 0 = First Year, 1 = Fourth Year\n",
    "df['Q33'] = pd.to_numeric(df['Q33'], errors='coerce')\n",
    "\n",
    "# Filter non-null rows for both columns\n",
    "filtered_df = df[['Q2', 'Q33']].dropna()\n",
    "\n",
    "# Cross-tabulate to get counts\n",
    "ctab_counts = pd.crosstab(filtered_df['Q33'], filtered_df['Q2'])\n",
    "ctab_counts = ctab_counts.reindex([1, 2, 3, 4, 5], fill_value=0)  # Ensure all 5 Likert levels are shown\n",
    "\n",
    "# Calculate percentages for each Likert level (row-wise percentages, for labels only)\n",
    "ctab_percent = ctab_counts.div(ctab_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Create stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create the stacked bars using counts\n",
    "bars = ctab_counts.plot(kind='bar', stacked=True, ax=ax, \n",
    "                        color=['#1f77b4', '#ff7f0e'], width=0.6, edgecolor='black')\n",
    "\n",
    "# Customize labels\n",
    "ax.set_title(\"Preferred Level of AI Integration by Academic Level\", fontsize=14)\n",
    "ax.set_xlabel(\"Preferred AI Integration Level\\n(1=No integration, 5=Full integration)\", fontsize=12)\n",
    "ax.set_ylabel(\"Count\", fontsize=12)\n",
    "ax.legend(['First Year', 'Fourth Year'], title=\"Academic Level\", loc='upper right')\n",
    "ax.set_xticklabels([f'Level {i}' for i in range(1, 6)], rotation=0)\n",
    "\n",
    "# Add percentage labels inside the bars\n",
    "for i, (idx, row) in enumerate(ctab_counts.iterrows()):\n",
    "    cumulative = 0\n",
    "    for j, (col, count) in enumerate(row.items()):\n",
    "        if count > 0:\n",
    "            y_pos = cumulative + count / 2\n",
    "            percent = ctab_percent.loc[idx, col]\n",
    "            if percent > 8:  # Show only if > 8% for clarity\n",
    "                ax.text(i, y_pos, f'{percent:.1f}%', \n",
    "                        ha='center', va='center', \n",
    "                        fontsize=10, color='white')\n",
    "            cumulative += count\n",
    "\n",
    "# Add total counts for each level at the top of bars\n",
    "max_height = ctab_counts.sum(axis=1).max()\n",
    "for i, (idx, row) in enumerate(ctab_counts.iterrows()):\n",
    "    total = row.sum()\n",
    "    ax.text(i, total + 0.05 * max_height, f'Total: {total}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Set y-axis to show counts with some space for total labels\n",
    "ax.set_ylim(0, max_height * 1.1)\n",
    "ax.set_ylabel(\"Count\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perform Chi-square test\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(ctab_counts.T)\n",
    "\n",
    "print(f\"\\nChi-square Test Results:\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Statistically significant difference (p < 0.05)\")\n",
    "else:\n",
    "    print(\"Result: No statistically significant difference (p ≥ 0.05)\")\n",
    "\n",
    "# Create summary table\n",
    "summary_table = pd.DataFrame()\n",
    "summary_table['Integration Level'] = [f'Level {i}' for i in range(1, 6)]\n",
    "summary_table['First Year Count'] = ctab_counts[0].values\n",
    "summary_table['Fourth Year Count'] = ctab_counts[1].values\n",
    "summary_table['Total'] = ctab_counts.sum(axis=1).values\n",
    "summary_table['First Year %'] = ctab_percent[0].round(1).values\n",
    "summary_table['Fourth Year %'] = ctab_percent[1].round(1).values\n",
    "\n",
    "print(summary_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877eabe0-868f-44cf-b46a-f4616429e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Translation dictionary\n",
    "translation_dict = {\n",
    "    \"ورش عمل تطبيقية\": \"Practical Workshops\",\n",
    "    \"دورات تدريبية عبر الإنترنت\": \"Online Training Courses\",\n",
    "    \"جلسات تدريب فردية\": \"Individual Training Sessions\",\n",
    "    \"التعلم الذاتي من المصادر المتاحة\": \"Self-learning from Available Resources\"\n",
    "}\n",
    "\n",
    "# Function to count and translate categories\n",
    "def count_and_translate(series):\n",
    "    all_choices = []\n",
    "    for response in series.dropna():\n",
    "        items = [item.strip() for item in str(response).split(\",\")]\n",
    "        for item in items:\n",
    "            translated = translation_dict.get(item, item)\n",
    "            all_choices.append(translated)\n",
    "    counts = Counter(all_choices)\n",
    "    total = sum(counts.values())\n",
    "    df_counts = pd.DataFrame(counts.items(), columns=[\"Response\", \"Count\"])\n",
    "    df_counts[\"Percent\"] = df_counts[\"Count\"] / total * 100\n",
    "    df_filtered = df_counts[df_counts[\"Count\"] >= 20].sort_values(by=\"Percent\", ascending=False)\n",
    "    return df_filtered\n",
    "\n",
    "# Process Q31 responses\n",
    "q31_cleaned = count_and_translate(df[\"Q31\"])\n",
    "\n",
    "# Vertical bar chart with percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(q31_cleaned[\"Response\"], q31_cleaned[\"Percent\"], color='skyblue', edgecolor='black')\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.title(\"Top Training Methods for Using AI\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.ylim(0, max(q31_cleaned[\"Percent\"]) * 1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print frequency table\n",
    "for index, row in q31_cleaned.iterrows():\n",
    "    print(f\"{row['Response']}: {row['Count']} ({row['Percent']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3b3fc-15fe-4cac-b7fd-ed55676c469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Arabic to English translation\n",
    "translation_q32 = {\n",
    "    \"إدماج الذكاء الاصطناعي في المناهج الدراسية\": \"Integrating AI into Curricula\",\n",
    "    \"توفير ورش عمل ودورات تدريبية للطلاب\": \"Providing Workshops and Training for Students\",\n",
    "    \"تشجيع استخدام أدوات الذكاء الاصطناعي في تحليل البيانات والأبحاث العلمية\": \"Encouraging Use of AI Tools in Research and Data Analysis\",\n",
    "    \"وضع سياسات واضحة بشأن استخدام الذكاء الاصطناعي لضمان الشفافية والأخلاقيات\": \"Establishing Clear AI Policies to Ensure Transparency and Ethics\"\n",
    "}\n",
    "\n",
    "# Process responses\n",
    "def process_q32(series):\n",
    "    flat_list = []\n",
    "    for response in series.dropna():\n",
    "        items = [item.strip() for item in str(response).split(\",\")]\n",
    "        for item in items:\n",
    "            translated = translation_q32.get(item, item)\n",
    "            flat_list.append(translated)\n",
    "    counts = Counter(flat_list)\n",
    "    total = sum(counts.values())\n",
    "    df_counts = pd.DataFrame(counts.items(), columns=[\"Response\", \"Count\"])\n",
    "    df_counts[\"Percent\"] = df_counts[\"Count\"] / total * 100\n",
    "    df_filtered = df_counts[df_counts[\"Count\"] >= 100].sort_values(by=\"Percent\", ascending=False)\n",
    "    return df_filtered\n",
    "\n",
    "# Apply processing\n",
    "q32_cleaned = process_q32(df[\"Q32\"])\n",
    "\n",
    "# Bar chart - vertical with percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(q32_cleaned[\"Response\"], q32_cleaned[\"Percent\"], color='skyblue', edgecolor='black')\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.title(\"Institutional Support Methods for AI Use\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "\n",
    "# Add percentage labels above bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.ylim(0, max(q32_cleaned[\"Percent\"]) * 1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Frequency output\n",
    "for index, row in q32_cleaned.iterrows():\n",
    "    print(f\"{row['Response']}: {row['Count']} ({row['Percent']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fa4ed-94ea-4e85-aea9-ce293b868232",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202a723-a837-4f67-a488-9b087b968978",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73ec21-5864-4007-b226-9e327b203f43",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f7057-5548-48ab-8beb-c52c8be4d6a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd982d9-0df3-46b0-9815-603c57b71196",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae1cc8-2281-4871-a935-c8c881f27b43",
   "metadata": {},
   "source": [
    "### Setting the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d6f24-f7ce-4a75-baec-5e631aa47f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9640db8c-bea7-41a8-b2d6-97ef0be035da",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fa548-811d-4349-a233-fd02399f3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sensitivity Analysis: Cronbach’s Alpha for Likert-scale Questions\n",
    "\n",
    "# Function to calculate Cronbach's alpha\n",
    "def cronbach_alpha(df_subset):\n",
    "    df_clean = df_subset.dropna()  # Drop rows with missing values\n",
    "    k = df_clean.shape[1]\n",
    "    item_variances = df_clean.var(axis=0, ddof=1)\n",
    "    total_variance = df_clean.sum(axis=1).var(ddof=1)\n",
    "    if total_variance == 0:\n",
    "        return np.nan  # Avoid division by zero\n",
    "    alpha = (k / (k - 1)) * (1 - item_variances.sum() / total_variance)\n",
    "    return alpha\n",
    "\n",
    "# Convert Likert columns to numeric if not already done\n",
    "likert_cols = (\n",
    "    [f'Q{i}' for i in range(9, 11)] +          # Q9 and Q10\n",
    "    ['Q12'] +                                  # Q12\n",
    "    [f'Q13_{i}' for i in range(1, 8)] +        # Q13.1 to Q13.7\n",
    "    [f'Q{i}' for i in range(14, 29)] +         # Q14 to Q28\n",
    "    ['Q30', 'Q33']                             # Extra items (not used here)\n",
    ")\n",
    "df[likert_cols] = df[likert_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Section 2: Use of AI Tools\n",
    "section2_cols = ['Q9', 'Q10', 'Q12'] + [f'Q13_{i}' for i in range(1, 8)]\n",
    "section2_alpha = cronbach_alpha(df[section2_cols])\n",
    "\n",
    "# Section 3: Impact of AI on Academic Performance\n",
    "section3_cols = [f'Q{i}' for i in range(14, 25)]\n",
    "section3_alpha = cronbach_alpha(df[section3_cols])\n",
    "\n",
    "# Section 4: Challenges in Using AI\n",
    "section4_cols = [f'Q{i}' for i in range(25, 29)]\n",
    "section4_alpha = cronbach_alpha(df[section4_cols])\n",
    "\n",
    "# Cronbach's Alpha for all Likert-scale variables (Full Model)\n",
    "full_model_cols = (\n",
    "    ['Q9', 'Q10', 'Q12'] +                    # Section 2\n",
    "    [f'Q13_{i}' for i in range(1, 8)] +       # Section 2\n",
    "    [f'Q{i}' for i in range(14, 29)]          # Sections 3 & 4\n",
    "    # You can optionally include Q30, Q33 if you want\n",
    ")\n",
    "full_model_alpha = cronbach_alpha(df[full_model_cols])\n",
    "\n",
    "# Display the results\n",
    "print(\"Cronbach's Alpha Results\")\n",
    "print(\"===========================\")\n",
    "print(f\"Section 2: Use of AI Tools → α = {section2_alpha:.3f}\")\n",
    "print(f\"Section 3: Impact of AI    → α = {section3_alpha:.3f}\")\n",
    "print(f\"Section 4: Challenges      → α = {section4_alpha:.3f}\")\n",
    "print(f\"Full Model (All Items)     → α = {full_model_alpha:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed4ee5-a08c-49cc-8766-207ea369b038",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d95aa-76af-4807-bb34-1678b433009e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f3962-5e5b-473c-8951-bc42073cd2b2",
   "metadata": {},
   "source": [
    "### 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed4c81-4441-44b4-b667-ea9df5600f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.contingency_tables import StratifiedTable\n",
    "import numpy as np\n",
    "\n",
    "# Define readable labels for Q5 and Q2\n",
    "gpa_labels = {0: '< 2.6', 1: '2.6 – <3.6', 2: '3.6 – 4.0'}\n",
    "year_labels = {0: 'First Year', 1: 'Fourth Year'}\n",
    "\n",
    "# Map readable labels for plotting\n",
    "df['GPA Range'] = df['Q5'].map(gpa_labels)\n",
    "df['Year'] = df['Q2'].map(year_labels)\n",
    "\n",
    "# Create cross-tabulation (counts)\n",
    "cross_tab = pd.crosstab(df['Year'], df['GPA Range'], margins=True)\n",
    "\n",
    "# Row-wise percentages\n",
    "cross_tab_percent = pd.crosstab(df['Year'], df['GPA Range'], normalize='index') * 100\n",
    "cross_tab_percent = cross_tab_percent.round(1)\n",
    "\n",
    "# Combine counts and percentages for display\n",
    "print(\"🔹 Cross-tabulation of Year (Q2) and GPA Range (Q5):\\n\")\n",
    "combined_table = cross_tab.drop('All').copy()\n",
    "for col in cross_tab.columns[:-1]:  # Exclude 'All' column\n",
    "    combined_table[col] = combined_table[col].astype(str) + \" (\" + cross_tab_percent[col].astype(str) + \"%)\"\n",
    "\n",
    "display(combined_table)\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['Year'], df['GPA Range']))\n",
    "print(\"\\nChi-square Test Result:\")\n",
    "print(f\"Chi-square statistic = {chi2:.3f}\")\n",
    "print(f\"Degrees of freedom = {dof}\")\n",
    "print(f\"P-value = {p:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"Result: There is a **statistically significant** association between Year and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no statistically significant** association between Year and GPA.\")\n",
    "\n",
    "\n",
    "# 🔹 Spearman's rank correlation (ordinal association)\n",
    "# Ensure both Q5 and Q12 are numeric for correlation\n",
    "spearman_corr, spearman_p = spearmanr(df['Q5'], df['Q2'], nan_policy='omit')\n",
    "\n",
    "print(\"\\nSpearman's Rank Correlation:\")\n",
    "print(f\"Spearman’s rho = {spearman_corr:.3f}\")\n",
    "print(f\"P-value = {spearman_p:.4f}\")\n",
    "\n",
    "if spearman_p < alpha:\n",
    "    print(\"Result: There is a **significant ordinal correlation** between Q2 and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no significant ordinal correlation** between Q2 and GPA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9763d53-435d-4694-a23a-ab3e9ac4a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define readable labels for Q5 and Q7\n",
    "gpa_labels = {\n",
    "    0: '< 2.6',\n",
    "    1: '2.6 – <3.6',\n",
    "    2: '3.6 – 4.0'\n",
    "}\n",
    "\n",
    "binary_labels = {\n",
    "    1: 'Yes',\n",
    "    0: 'No'\n",
    "}\n",
    "\n",
    "# Map readable labels for plotting\n",
    "df['GPA Range'] = df['Q5'].map(gpa_labels)\n",
    "df['Q7 Response'] = df['Q7'].map(binary_labels)\n",
    "\n",
    "# Create cross-tabulation (counts)\n",
    "cross_tab = pd.crosstab(df['Q7 Response'], df['GPA Range'], margins=True)\n",
    "\n",
    "# Row-wise percentages\n",
    "cross_tab_percent = pd.crosstab(df['Q7 Response'], df['GPA Range'], normalize='index') * 100\n",
    "cross_tab_percent = cross_tab_percent.round(1)\n",
    "\n",
    "# Combine counts and percentages for display\n",
    "print(\"🔹 Cross-tabulation of Q7 (Yes/No) and GPA Range (Q5):\\n\")\n",
    "combined_table = cross_tab.drop('All').copy()\n",
    "for col in cross_tab.columns[:-1]:  # Exclude 'All' column\n",
    "    combined_table[col] = combined_table[col].astype(str) + \" (\" + cross_tab_percent[col].astype(str) + \"%)\"\n",
    "\n",
    "display(combined_table)\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['Q7 Response'], df['GPA Range']))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nChi-square Test Result:\")\n",
    "print(f\"Chi-square statistic = {chi2:.3f}\")\n",
    "print(f\"Degrees of freedom = {dof}\")\n",
    "print(f\"P-value = {p:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"Result: There is a **statistically significant** association between Q7 Response and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no statistically significant** association between Q7 Response and GPA.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c7d20-3ba3-4a11-9efd-8b1a6809334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define readable labels for Q5 and Q12 (assuming Likert scale 1-5)\n",
    "gpa_labels = {\n",
    "    0: '< 2.6',\n",
    "    1: '2.6 – <3.6',\n",
    "    2: '3.6 – 4.0'\n",
    "}\n",
    "\n",
    "likert_labels = {\n",
    "    1: 'Strongly Disagree',\n",
    "    2: 'Disagree',\n",
    "    3: 'Neutral',\n",
    "    4: 'Agree',\n",
    "    5: 'Strongly Agree'\n",
    "}\n",
    "\n",
    "# Map readable labels for plotting\n",
    "df['GPA Range'] = df['Q5'].map(gpa_labels)\n",
    "df['Q12 Response'] = df['Q12'].map(likert_labels)\n",
    "\n",
    "# Create cross-tabulation (counts)\n",
    "cross_tab = pd.crosstab(df['Q12 Response'], df['GPA Range'], margins=True)\n",
    "\n",
    "# Row-wise percentages\n",
    "cross_tab_percent = pd.crosstab(df['Q12 Response'], df['GPA Range'], normalize='index') * 100\n",
    "cross_tab_percent = cross_tab_percent.round(1)\n",
    "\n",
    "# Combine counts and percentages for display\n",
    "print(\"🔹 Cross-tabulation of Q12 (Likert) and GPA Range (Q5):\\n\")\n",
    "combined_table = cross_tab.drop('All').copy()\n",
    "for col in cross_tab.columns[:-1]:  # Exclude 'All' column\n",
    "    combined_table[col] = combined_table[col].astype(str) + \" (\" + cross_tab_percent[col].astype(str) + \"%)\"\n",
    "\n",
    "display(combined_table)\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['Q12 Response'], df['GPA Range']))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nChi-square Test Result:\")\n",
    "print(f\"Chi-square statistic = {chi2:.3f}\")\n",
    "print(f\"Degrees of freedom = {dof}\")\n",
    "print(f\"P-value = {p:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"Result: There is a **statistically significant** association between Q12 Response and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no statistically significant** association between Q12 Response and GPA.\")\n",
    "\n",
    "# 🔹 Spearman's rank correlation (ordinal association)\n",
    "# Ensure both Q5 and Q12 are numeric for correlation\n",
    "spearman_corr, spearman_p = spearmanr(df['Q5'], df['Q12'], nan_policy='omit')\n",
    "\n",
    "print(\"\\nSpearman's Rank Correlation:\")\n",
    "print(f\"Spearman’s rho = {spearman_corr:.3f}\")\n",
    "print(f\"P-value = {spearman_p:.4f}\")\n",
    "\n",
    "if spearman_p < alpha:\n",
    "    print(\"Result: There is a **significant ordinal correlation** between Q12 and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no significant ordinal correlation** between Q12 and GPA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a74eb-883f-41e3-918c-95b49a456fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, spearmanr\n",
    "\n",
    "# Define readable labels for Q5 and Q27 (assuming Likert scale 1-5)\n",
    "gpa_labels = {\n",
    "    0: '< 2.6',\n",
    "    1: '2.6 – <3.6',\n",
    "    2: '3.6 – 4.0'\n",
    "}\n",
    "\n",
    "likert_labels = {\n",
    "    1: 'Strongly Disagree',\n",
    "    2: 'Disagree',\n",
    "    3: 'Neutral',\n",
    "    4: 'Agree',\n",
    "    5: 'Strongly Agree'\n",
    "}\n",
    "\n",
    "# Map readable labels for plotting\n",
    "df['GPA Range'] = df['Q5'].map(gpa_labels)\n",
    "df['Q27 Response'] = df['Q27'].map(likert_labels)\n",
    "\n",
    "# Create cross-tabulation (counts)\n",
    "cross_tab = pd.crosstab(df['Q27 Response'], df['GPA Range'], margins=True)\n",
    "\n",
    "# Row-wise percentages\n",
    "cross_tab_percent = pd.crosstab(df['Q27 Response'], df['GPA Range'], normalize='index') * 100\n",
    "cross_tab_percent = cross_tab_percent.round(1)\n",
    "\n",
    "# Combine counts and percentages for display\n",
    "print(\"🔹 Cross-tabulation of Q27 (Likert) and GPA Range (Q5):\\n\")\n",
    "combined_table = cross_tab.drop('All').copy()\n",
    "for col in cross_tab.columns[:-1]:  # Exclude 'All' column\n",
    "    combined_table[col] = combined_table[col].astype(str) + \" (\" + cross_tab_percent[col].astype(str) + \"%)\"\n",
    "\n",
    "display(combined_table)\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['Q27 Response'], df['GPA Range']))\n",
    "\n",
    "print(\"\\nChi-square Test Result:\")\n",
    "print(f\"Chi-square statistic = {chi2:.3f}\")\n",
    "print(f\"Degrees of freedom = {dof}\")\n",
    "print(f\"P-value = {p:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"Result: There is a **statistically significant** association between Q27 Response and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no statistically significant** association between Q27 Response and GPA.\")\n",
    "\n",
    "# 🔹 Spearman's rank correlation (ordinal association)\n",
    "# Ensure both Q5 and Q27 are numeric for correlation\n",
    "spearman_corr, spearman_p = spearmanr(df['Q5'], df['Q27'], nan_policy='omit')\n",
    "\n",
    "print(\"\\nSpearman's Rank Correlation:\")\n",
    "print(f\"Spearman’s rho = {spearman_corr:.3f}\")\n",
    "print(f\"P-value = {spearman_p:.4f}\")\n",
    "\n",
    "if spearman_p < alpha:\n",
    "    print(\"Result: There is a **significant ordinal correlation** between Q27 and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no significant ordinal correlation** between Q27 and GPA.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ed04a-0b43-42f5-a915-e5c880d9c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Define readable labels\n",
    "gpa_labels = {\n",
    "    0: '< 2.6',\n",
    "    1: '2.6 – <3.6',\n",
    "    2: '3.6 – 4.0'\n",
    "}\n",
    "\n",
    "q4_labels = {\n",
    "    1: 'Political Science',\n",
    "    2: 'Economics',\n",
    "    3: 'Statistics'\n",
    "}\n",
    "\n",
    "# Apply mappings\n",
    "df['GPA Range'] = df['Q5'].map(gpa_labels)\n",
    "df['Academic Level'] = df['Q4'].map(q4_labels)\n",
    "\n",
    "# Remove rows with 'Not Yet Specialized' (Q4 = 0)\n",
    "df_filtered = df[df['Q4'] != 0].copy()\n",
    "\n",
    "# Create cross-tabulation (counts)\n",
    "cross_tab = pd.crosstab(df_filtered['Academic Level'], df_filtered['GPA Range'], margins=True)\n",
    "\n",
    "# Row-wise percentages\n",
    "cross_tab_percent = pd.crosstab(df_filtered['Academic Level'], df_filtered['GPA Range'], normalize='index') * 100\n",
    "cross_tab_percent = cross_tab_percent.round(1)\n",
    "\n",
    "# Combine counts and percentages for display\n",
    "print(\"Cross-tabulation of Academic Level and GPA Range :\\n\")\n",
    "combined_table = cross_tab.drop('All').copy()\n",
    "for col in cross_tab.columns[:-1]:  # Exclude 'All' column\n",
    "    combined_table[col] = combined_table[col].astype(str) + \" (\" + cross_tab_percent[col].astype(str) + \"%)\"\n",
    "\n",
    "display(combined_table)\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(pd.crosstab(df_filtered['Academic Level'], df_filtered['GPA Range']))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nChi-square Test Result:\")\n",
    "print(f\"Chi-square statistic = {chi2:.3f}\")\n",
    "print(f\"Degrees of freedom = {dof}\")\n",
    "print(f\"P-value = {p:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"Result: There is a **statistically significant** association between Academic Level and GPA.\")\n",
    "else:\n",
    "    print(\"Result: There is **no statistically significant** association between Academic Level and GPA.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42480401-2a8f-40f1-9043-bab5cdc8993b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821f1cc-b6e6-475d-bfce-9fabea888c6f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acb64e-d451-4554-87e1-403ceff30f19",
   "metadata": {},
   "source": [
    "### 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc509a64-b6c2-4051-907c-3a5d2df7938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity\n",
    "\n",
    "# Select Likert-scale columns\n",
    "likert_cols = (\n",
    "    [f'Q{i}' for i in range(9, 11)] +\n",
    "    [f'Q13_{i}' for i in range(1, 8)] +\n",
    "    [f'Q{i}' for i in range(14, 29)] +\n",
    "    ['Q30', 'Q33']\n",
    ")\n",
    "\n",
    "# Clean the data\n",
    "df_likert = df_yes[likert_cols].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "# Bartlett's Test\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(df_likert)\n",
    "print(f\"Bartlett’s Test: chi-square = {chi_square_value:.2f}, p-value = {p_value:.4f}\")\n",
    "\n",
    "# KMO Test\n",
    "kmo_all, kmo_model = calculate_kmo(df_likert)\n",
    "print(f\"Kaiser-Meyer-Olkin (KMO) Test: KMO = {kmo_model:.4f}\")\n",
    "\n",
    "# Run factor analysis without rotation to get eigenvalues\n",
    "fa = FactorAnalyzer(n_factors=len(likert_cols), rotation=None)\n",
    "fa.fit(df_likert)\n",
    "\n",
    "# Get and show eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "eigenvalues_df = pd.DataFrame({'Factor': range(1, len(ev)+1), 'Eigenvalue': ev})\n",
    "print(\"\\nAll Eigenvalues:\\n\")\n",
    "print(eigenvalues_df)\n",
    "\n",
    "# Scree Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=eigenvalues_df, x='Factor', y='Eigenvalue', marker='o')\n",
    "plt.axhline(1, color='red', linestyle='--')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factor Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decide number of factors (e.g., factors with eigenvalue > 1)\n",
    "n_factors = sum(ev > 1)  # or set manually\n",
    "\n",
    "# Show unrotated factor loadings\n",
    "fa_unrotated = FactorAnalyzer(n_factors=n_factors, rotation=None)\n",
    "fa_unrotated.fit(df_likert)\n",
    "loadings_unrotated = pd.DataFrame(fa_unrotated.loadings_, index=likert_cols, columns=[f'Factor{i+1}' for i in range(n_factors)])\n",
    "print(\"\\nUnrotated Factor Loadings:\\n\")\n",
    "print(loadings_unrotated.round(2))\n",
    "\n",
    "# Run factor analysis again with rotation (e.g., varimax)\n",
    "fa_rotated = FactorAnalyzer(n_factors=n_factors, rotation='varimax')\n",
    "fa_rotated.fit(df_likert)\n",
    "loadings_rotated = pd.DataFrame(fa_rotated.loadings_, index=likert_cols, columns=[f'Factor{i+1}' for i in range(n_factors)])\n",
    "print(\"\\nRotated Factor Loadings (Varimax):\\n\")\n",
    "print(loadings_rotated.round(2))\n",
    "\n",
    "# Variance Explained by Each Factor\n",
    "variance_df = pd.DataFrame({\n",
    "    'SS Loadings': fa_rotated.get_factor_variance()[0],\n",
    "    'Proportion Var': fa_rotated.get_factor_variance()[1],\n",
    "    'Cumulative Var': fa_rotated.get_factor_variance()[2],\n",
    "}, index=[f'Factor{i+1}' for i in range(n_factors)])\n",
    "print(\"\\nVariance Explained by Each Factor:\\n\")\n",
    "print(variance_df.round(3))\n",
    "\n",
    "# Communalities\n",
    "communalities_df = pd.DataFrame(fa_rotated.get_communalities(), index=likert_cols, columns=['Communality'])\n",
    "print(\"\\nCommunalities:\\n\")\n",
    "print(communalities_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e8079-9224-44d2-9466-cdbedb10cb9d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b4e28-9183-4584-98d2-eb328f6f462b",
   "metadata": {},
   "source": [
    "### 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10980532-cc78-4c1a-89d6-c449f714e0c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e1553-860f-4d60-aad0-4011ab758ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Variance Inflation Factor (VIF) – Multicollinearity Check\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create factor columns based on rotated loadings\n",
    "factor_items = {}\n",
    "for item in loadings_rotated.index:\n",
    "    factor = loadings_rotated.loc[item].abs().idxmax()\n",
    "    factor_items.setdefault(factor, []).append(item)\n",
    "factor_items = dict(sorted(factor_items.items()))\n",
    "\n",
    "for factor, cols in factor_items.items():\n",
    "    df[factor] = df[cols].mean(axis=1)\n",
    "\n",
    "# Prepare data\n",
    "model_cols = ['Q1', 'Q2', 'Q3', 'Q4', 'Q6', 'Q7', 'Q11'] + [f'Factor{i}' for i in range(1, 8)] + ['Q5']\n",
    "df_model = df[model_cols].dropna()\n",
    "\n",
    "# Compute VIFs\n",
    "X_vif = df_model.drop(columns='Q5')\n",
    "X_vif_const = sm.add_constant(X_vif)\n",
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_vif_const.columns,\n",
    "    'VIF': [variance_inflation_factor(X_vif_const.values, i) for i in range(X_vif_const.shape[1])]\n",
    "})\n",
    "print(\"\\n--- Variance Inflation Factor (VIF) ---\")\n",
    "print(vif_data)\n",
    "\n",
    "# Step 2: Fit the Ordered Logistic Regression\n",
    "X = df_model.drop(columns='Q5')\n",
    "y = df_model['Q5'].astype(int)\n",
    "ord_model = OrderedModel(y, X, distr='logit')\n",
    "res = ord_model.fit(method='bfgs', maxiter=100)\n",
    "print(res.summary())\n",
    "\n",
    "# Step 3: Predicting and Evaluating Performance\n",
    "y_pred = res.model.predict(res.params).argmax(axis=1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "print(\"\\n--- Prediction Evaluation ---\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y, y_pred))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y, y_pred))\n",
    "\n",
    "# Step 4: Write the Regression Equation\n",
    "params = res.params\n",
    "cutpoints = {k: v for k, v in params.items() if '/' in k}\n",
    "betas = {k: v for k, v in params.items() if '/' not in k}\n",
    "print(\"\\n--- Regression Function ---\")\n",
    "eq = \"logit(P(Y ≤ j)) = τ_j - (\" + ' + '.join([f\"{coef:.3f}*{name}\" for name, coef in betas.items()]) + ')'\n",
    "print(eq)\n",
    "print(\"\\nThresholds (Cutpoints):\")\n",
    "for k, v in cutpoints.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "# Step 5: Proportional Odds Assumption Test (via Likelihood Ratio)\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import chi2\n",
    "df_model['Q5'] = y\n",
    "df_model['Q5_cat'] = df_model['Q5'].astype('category').cat.codes\n",
    "\n",
    "formula = 'Q5_cat ~ ' + ' + '.join(X.columns)\n",
    "mnlogit_model = smf.mnlogit(formula=formula, data=df_model).fit(maxiter=100, disp=False)\n",
    "lr_stat = 2 * (mnlogit_model.llf - res.llf)\n",
    "df_diff = mnlogit_model.df_model - res.df_model\n",
    "p_value = chi2.sf(lr_stat, df_diff)\n",
    "print(\"\\n--- Proportional Odds Assumption Test ---\")\n",
    "print(f\"LR Statistic: {lr_stat:.3f}\")\n",
    "print(f\"Degrees of Freedom: {df_diff}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "print(\"→ Assumption\", \"likely violated.\" if p_value < 0.05 else \"likely satisfied.\")\n",
    "\n",
    "#print(\"\\n--- Linearity Assumption Test via Box-Tidwell---\")\n",
    "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "#\n",
    "#for var in continuous_vars:\n",
    "#    try:\n",
    "#        formula = f'Q5_cat ~ {var} + {var}_log'\n",
    "#        model = smf.mnlogit(formula=formula, data=df_bt).fit(maxiter=100, disp=False)\n",
    "#        \n",
    "#        # List all p-value keys\n",
    "#        pval_keys = list(model.pvalues.index)\n",
    "#        \n",
    "#        # Try to find the _log term key with regex (case insensitive)\n",
    "#        pattern = re.compile(re.escape(var) + r'.*_log', re.IGNORECASE)\n",
    "#        matched_keys = [key for key in pval_keys if pattern.search(key)]\n",
    "#        \n",
    "#        if matched_keys:\n",
    "#            # Take the first matching key\n",
    "#            key = matched_keys[0]\n",
    "#            p = model.pvalues[key]\n",
    "#            print(f\"{var}: p-value for '{key}' = {p:.4f} →\", \"Violated\" if p < 0.05 else \"Satisfied\")\n",
    "#        else:\n",
    "#            print(f\"{var}: log term not found in model p-values. Keys found: {pval_keys}\")\n",
    "#    \n",
    "#    except Exception as e:\n",
    "#        print(f\"{var}: Skipped due to error -> {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adebead-c474-4224-b923-2abb1fe68524",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c3668-e007-4cea-a899-05f721e614ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b15d0-89be-46d6-9856-843b22253d0e",
   "metadata": {},
   "source": [
    "### 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92aa178-b244-4ad1-a463-a16ddaaa94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Define your full model\n",
    "model_cols = ['Q1', 'Q2', 'Q3', 'Q4', 'Q6', 'Q7', 'Q11'] + [f'Factor{i}' for i in range(1, 8)] + ['Q5']\n",
    "df_model = df[model_cols].dropna()\n",
    "\n",
    "X = df_model.drop(columns='Q5')\n",
    "y = df_model['Q5'].astype(int)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Class weights (for XGB & CatBoost)\n",
    "classes = np.unique(y_train)\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "sample_weights = y_train.map(class_weight_dict)\n",
    "\n",
    "# ===============================\n",
    "# HELPER FUNCTION TO PLOT CONFUSION MATRIX\n",
    "def plot_confusion(cm, title):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===============================\n",
    "# MODEL 1: RANDOM FOREST\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Random Forest Results ---\")\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Confusion Matrix:\\n\", cm_rf)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred_rf))\n",
    "plot_confusion(cm_rf, \"Random Forest Confusion Matrix\")\n",
    "\n",
    "# ===============================\n",
    "# MODEL 2: XGBOOST\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(classes),\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n--- XGBoost Results ---\")\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(\"Confusion Matrix:\\n\", cm_xgb)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred_xgb))\n",
    "plot_confusion(cm_xgb, \"XGBoost Confusion Matrix\")\n",
    "\n",
    "# ===============================\n",
    "# MODEL 3: CATBOOST\n",
    "cat_class_weights = [class_weight_dict[c] for c in sorted(class_weight_dict.keys())]\n",
    "\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    random_seed=42,\n",
    "    verbose=0,\n",
    "    class_weights=cat_class_weights\n",
    ")\n",
    "cat.fit(X_train, y_train)\n",
    "y_pred_cat = cat.predict(X_test)\n",
    "\n",
    "print(\"\\n--- CatBoost Results ---\")\n",
    "cm_cat = confusion_matrix(y_test, y_pred_cat)\n",
    "print(\"Confusion Matrix:\\n\", cm_cat)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_cat))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_cat))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred_cat))\n",
    "plot_confusion(cm_cat, \"CatBoost Confusion Matrix\")\n",
    "\n",
    "# ===============================\n",
    "# FEATURE IMPORTANCES (CatBoost)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming cat and X are already defined\n",
    "feature_importances = cat.get_feature_importance()\n",
    "features = X.columns\n",
    "\n",
    "# Create a DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=True)  # Changed to ascending for better vertical display\n",
    "\n",
    "# Plot vertical barplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x='Feature', y='Importance', data=importance_df, orient='v')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for i, (value) in enumerate(importance_df['Importance']):\n",
    "    ax.text(i, value + 0.01, f'{value:.2f}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.title('CatBoost Feature Importances')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732c72a-6187-437f-a6aa-5e2c8f8cce82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b7b53-e70b-4d6d-881f-671c348f9a77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2f411-d696-40fc-bc2b-3b1c9a771a84",
   "metadata": {},
   "source": [
    "### 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c75bb7-4eab-477b-91d3-9a7333e98a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLUSTER ANALYSIS BASED ONLY ON FACTORS ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# STEP 1: Prepare data with only factors + GPA\n",
    "#factor_cols = [f'Factor{i}' for i in range(1, 8)] + ['Q5']\n",
    "factor_cols = ['Q1', 'Q2', 'Q3', 'Q4', 'Q6', 'Q7', 'Q11'] + [f'Factor{i}' for i in range(1, 8)] + ['Q5']\n",
    "df_model = df[factor_cols].dropna()\n",
    "X = df_model.drop(columns='Q5')\n",
    "y = df_model['Q5'].astype(int)\n",
    "\n",
    "# STEP 2: Standardize the factor scores\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# STEP 3: Elbow Method to find optimal k\n",
    "inertia = []\n",
    "K_range = range(1, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertia, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method: Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# STEP 4: Apply KMeans with chosen k\n",
    "#k_optimal = 2\n",
    "#kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "#clusters = kmeans.fit_predict(X_scaled)\n",
    "#df_model['Cluster'] = clusters\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "clusters = gmm.fit_predict(X_scaled)\n",
    "df_model['Cluster'] = clusters\n",
    "\n",
    "print(\"Cluster Counts:\")\n",
    "print(df_model['Cluster'].value_counts().sort_index())\n",
    "\n",
    "# STEP 5: PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "df_model['PCA1'] = X_pca[:, 0]\n",
    "df_model['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_model, x='PCA1', y='PCA2', hue='Cluster', palette='Set1', s=100, alpha=0.7)\n",
    "plt.title('Cluster Visualization (PCA)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# STEP 6: Describe Cluster Profiles\n",
    "#cluster_means = df_model.groupby('Cluster')[[f'Factor{i}' for i in range(1, 8)]].mean()\n",
    "cluster_means = df_model.groupby('Cluster')[['Q1', 'Q2', 'Q3', 'Q4', 'Q6', 'Q7', 'Q11'] + [f'Factor{i}' for i in range(1, 8)]].mean()\n",
    "print(\"\\nCluster Profiles (Mean Factor Scores):\")\n",
    "print(cluster_means.round(2))\n",
    "\n",
    "# STEP 7: GPA Distribution per Cluster (%)\n",
    "gpa_dist = pd.crosstab(df_model['Cluster'], df_model['Q5'], normalize='index') * 100\n",
    "print(\"\\nGPA Distribution by Cluster (%):\")\n",
    "print(gpa_dist.round(1))\n",
    "\n",
    "# STEP 8: Stacked Bar Plot with Percent Labels\n",
    "ax = gpa_dist.plot(kind='bar', stacked=True, colormap='Set2', figsize=(10, 6))\n",
    "plt.title(\"GPA Distribution Within Each Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Percentage of Students\")\n",
    "plt.legend(title=\"GPA Category\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, row in enumerate(gpa_dist.values):\n",
    "    cum_height = 0\n",
    "    for j, val in enumerate(row):\n",
    "        if val > 0:\n",
    "            ax.text(i, cum_height + val / 2, f\"{val:.1f}%\", ha='center', va='center', fontsize=12)\n",
    "            cum_height += val\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30441ce2-73c2-49c7-afe5-25543aeff845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
